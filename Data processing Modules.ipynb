{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Preprocessing Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Process\n",
    "\n",
    "\n",
    "1. load as DataFrame\n",
    "2. extract date into [\"year\",\"month\",\"day\"]\n",
    "3. drop columns that have more than *threshold amount of missing values<br/>\n",
    "    store the dropped columns and return the list of dropped columns\n",
    "4. Input missing values<br/>\n",
    "    Continuous variables - median<br/>\n",
    "    Categorical variables - seperate category\n",
    "5. Handle categorical variables<br/>\n",
    "    identify categorical variables<br/>\n",
    "    convert to pandas \"catergory\" <br/>\n",
    "    encode with pandas.cat.codes<br/>\n",
    "6. Split into X and y\n",
    "7. Split into training, validation set\n",
    "8. Train model\n",
    "9. evaluate model performance\n",
    "10. Hyperparameter tuning\n",
    "11. parameter inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_all(df):\n",
    "    \"\"\"\n",
    "    Set max rows and columns to display\n",
    "    \"\"\"\n",
    "    with pd.option_context(\"display.max_rows\",1000):\n",
    "        with pd.option_context(\"display.max_columns\",1000):\n",
    "            display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process datetime column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_date(df,col,attr=[\"year\",\"month\",\"day\"],drop=True):\n",
    "    \"\"\"\n",
    "    Process datatime column\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : \n",
    "    DataFrame\n",
    "    \n",
    "    col : \n",
    "    single column name that contain date information\n",
    "    \n",
    "    attr : default= [\"year\",\"month\",\"day\"]\n",
    "    attribute you wish to extract from the datetime column\n",
    "    \n",
    "    drop : default=True\n",
    "    if True, drop the datetime column after processing\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    if not np.issubdtype(df[col],np.datetime64):\n",
    "        df[col] = pd.to_datetime(df[col],infer_datetime_format=True)\n",
    "    \n",
    "    for ea in attr:\n",
    "        df[col+\"_\"+ea] = getattr(df[col].dt,ea)\n",
    "    \n",
    "    if drop:\n",
    "        df.drop(col,axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Columns with missing values over threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_missing(df,threshold=0.5,drop_cols=[]):\n",
    "    \"\"\"\n",
    "    Process missing columns\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : \n",
    "    DataFrame\n",
    "    \n",
    "    threshold : default=0.5\n",
    "    amount of missing value in columns required to drop the column\n",
    "    \n",
    "    drop_cols : default=[]\n",
    "    list of columns to be dropped. If not given, function will drop column based on amount of missing values\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    Dataframe with the columns dropped\n",
    "    Dropped columns name as a list\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if not drop_cols:\n",
    "        rows = len(df)\n",
    "        num_nonna = round((1-threshold) * rows,0)\n",
    "        for k,v in (df.isnull().sum()/rows).items():\n",
    "            if v>threshold:\n",
    "                drop_cols.append(k)\n",
    "        \n",
    "        d= df.dropna(axis=1,thresh = num_nonna)\n",
    "    else:\n",
    "        d= df.drop(drop_cols,axis=1)\n",
    "            \n",
    "    \n",
    "    return d,drop_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_miss(df,missing_val={}):\n",
    "    \"\"\"\n",
    "    Process missing values\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : \n",
    "    DataFrame\n",
    "    \n",
    "    missing_val : default={}\n",
    "    Dict with column names as keys, value to replace as values. If not given, function will replace numeric missing\n",
    "    values with median of the respective column\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    Dataframe with missing values filled \n",
    "    missing_val with columns as key and median of the respective column as values\n",
    "    \n",
    "    \"\"\"\n",
    "    d= df.copy()\n",
    "    if not missing_val:\n",
    "        missing_val = fill_numeric(d,missing_val)\n",
    "    \n",
    "    else:\n",
    "        for k,v in missing_val.items():\n",
    "            if d[k].isnull().sum():\n",
    "                d[k].fillna(v,inplace=True)\n",
    "        \n",
    "        if d.isnull().sum().sum():\n",
    "            for col in d.columns:\n",
    "                missing_val = fill_numeric(d,missing_val)\n",
    "\n",
    "    return d,missing_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_numeric(df,missing_val):\n",
    "    for col in df.columns:\n",
    "        if pd.api.types.is_numeric_dtype(df[col].dtypes):\n",
    "            if df[col].isnull().sum():\n",
    "                df[col].fillna(df[col].median(),inplace=True)\n",
    "                missing_val[col] = df[col].median()\n",
    "    return missing_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_cat(df,cat_cols=[]):\n",
    "    if cat_cols:\n",
    "        for col in cat_cols:\n",
    "                df[col] = df[col].astype(\"category\")\n",
    "    else:\n",
    "        obj_columns = df.select_dtypes(['object']).columns\n",
    "        for obj in obj_columns:\n",
    "            df[obj] = df[obj].astype('category')\n",
    "            cat_cols.append(obj)\n",
    "    return df, cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_cat(df,cat_dict={}):\n",
    "    if cat_dict:\n",
    "        for k,v in cat_dict.items():\n",
    "            df[k] = df[k].cat.set_categories(v)\n",
    "    else:\n",
    "        for col in df.columns:\n",
    "            if df[col].dtypes.name ==\"category\":\n",
    "                cat_dict[col] = df[col].cat.categories\n",
    "    return cat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_dummies(df,cat_cols,max_cardi):\n",
    "    cardi_cols = []\n",
    "    for col in cat_cols:\n",
    "        if len(df[col].cat.categories) <= max_cardi:\n",
    "            cardi_cols.append(col)\n",
    "    \n",
    "    df = pd.get_dummies(df,columns = cardi_cols,prefix=cardi_cols,drop_first=True)\n",
    "    \n",
    "    return df, cardi_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_codes(df,cat_cols):\n",
    "    for col in cat_cols:\n",
    "        df[col] = df[col].cat.codes+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_cat(df,cat_cols=[],cat_dict={},max_cardi=None):\n",
    "    \"\"\"\n",
    "    Process categorical variables\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: \n",
    "    DataFrame\n",
    "    \n",
    "    cat_cols : default=[]\n",
    "    list of pre-determined categorical variables\n",
    "    \n",
    "    cat_dict : default={}\n",
    "    Dict with categorical variables as keys and pandas.Series.cat.categories as values. If not given, cat_dict is\n",
    "    generated with for every categorical columns\n",
    "    \n",
    "    max_cardi : default=None\n",
    "    maximum cardinality of the categorical variables. Which is the number of class in the categorical features.\n",
    "    Categories variables with cardinality less or equal to max_cardi will be onehotencoded to produce dummies variables\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    Dataframe with categorical variables processed\n",
    "    cat_dict with categorical columns as key and respective pandas.Series.cat.categories as values\n",
    "    \n",
    "    \"\"\"\n",
    "    d = df.copy()\n",
    "    \n",
    "    d, cat_cols = convert_cat(d,cat_cols)\n",
    "\n",
    "    cat_dict = set_cat(d,cat_dict)\n",
    "    \n",
    "    if max_cardi:\n",
    "        d,cardi_cols = gen_dummies(d,cat_cols,max_cardi)\n",
    "        cat_cols = list(set(cat_cols) - set(cardi_cols))\n",
    "    \n",
    "    cat_codes(d,cat_cols)\n",
    "    \n",
    "    return d, cat_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train_valid splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_split(df,num_valid,shuffle=False):\n",
    "    \"\"\"\n",
    "    Split df into training and validation set\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : \n",
    "    DataFrame\n",
    "    \n",
    "    num_valid : \n",
    "    number of samples needed in validation set\n",
    "    \n",
    "    shuffle : default=False\n",
    "    Shuffle the rows to randomly sample training and validation sets\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    Training and validation set respectively\n",
    "    \n",
    "    \"\"\"\n",
    "    if shuffle:\n",
    "        df = df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    n_trn = len(df) - num_valid\n",
    "    n_train = df[:n_trn]\n",
    "    n_valid = df[n_trn:]\n",
    "    \n",
    "    return n_train, n_valid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
